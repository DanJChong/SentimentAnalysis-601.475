{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM - Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXponN4wHIOV"
      },
      "source": [
        "# ML Final Project\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHzGm6ngHSzC",
        "outputId": "acc5ba43-21d5-424e-f07e-a1fa19ed1371"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n-mA47KHn-w"
      },
      "source": [
        "## Imports\n",
        "import numpy as np\n",
        "# from sklearn.linear_model import train_test_split"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVQegsJXHcFt"
      },
      "source": [
        "## Load data\n",
        "path = '/content/drive/MyDrive/ML Final Project/data/'\n",
        "data_amzn = np.loadtxt(path + 'amazon_cells_labelled.txt', dtype='object',delimiter='\\t')\n",
        "x_amzn = [data[0] for data in data_amzn]\n",
        "y_amzn = [int(data[1]) for data in data_amzn]\n",
        "\n",
        "data_imdb = np.loadtxt(path + 'imdb_labelled.txt', dtype='object',delimiter='\\t')\n",
        "x_imdb = [data[0] for data in data_imdb]\n",
        "y_imdb = [int(data[1]) for data in data_imdb]\n",
        "\n",
        "data_yelp = np.loadtxt(path + 'yelp_labelled.txt', dtype='object',delimiter='\\t')\n",
        "x_yelp = [data[0] for data in data_yelp]\n",
        "y_yelp = [int(data[1]) for data in data_yelp]\n",
        "\n",
        "train_cutoff = 800\n",
        "dev_cutoff = 900\n",
        "\n",
        "x_train = x_amzn[0:train_cutoff] + x_imdb[0:train_cutoff] + x_yelp[0:train_cutoff]\n",
        "y_train = y_amzn[0:train_cutoff] + y_imdb[0:train_cutoff] + y_yelp[0:train_cutoff]\n",
        "\n",
        "x_dev = x_amzn[train_cutoff:dev_cutoff] + x_imdb[train_cutoff:dev_cutoff] + x_yelp[train_cutoff:dev_cutoff]\n",
        "y_dev = y_amzn[train_cutoff:dev_cutoff] + y_imdb[train_cutoff:dev_cutoff] + y_yelp[train_cutoff:dev_cutoff]\n",
        "\n",
        "x_test = x_amzn[dev_cutoff:] + x_imdb[dev_cutoff:] + x_yelp[dev_cutoff:]\n",
        "y_test = y_amzn[dev_cutoff:] + y_imdb[dev_cutoff:] + y_yelp[dev_cutoff:]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9d9JjjHd3M"
      },
      "source": [
        "## Preprocess data\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "X_dev = vectorizer.transform(x_dev)\n",
        "X_test = vectorizer.transform(x_test)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsS3FjgDl9Cs"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from keras.preprocessing import sequence\r\n",
        "\r\n",
        "vocab_size = 10000\r\n",
        "encoded_training = [tf.keras.preprocessing.text.one_hot(d, vocab_size) for d in x_train] \r\n",
        "encoded_dev = [tf.keras.preprocessing.text.one_hot(d, vocab_size) for d in x_dev]\r\n",
        "encoded_test = [tf.keras.preprocessing.text.one_hot(d, vocab_size) for d in x_test]\r\n",
        "max_words = 500\r\n",
        "\r\n",
        "X_train = sequence.pad_sequences(encoded_training, maxlen=max_words)\r\n",
        "X_dev = sequence.pad_sequences(encoded_dev, maxlen=max_words)\r\n",
        "X_test = sequence.pad_sequences(encoded_test, maxlen= max_words)\r\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v4kOo7oi8PY"
      },
      "source": [
        "from keras import Sequential\r\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\r\n",
        "\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "\r\n",
        "embedding_size=64\r\n",
        "model=Sequential()\r\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_words))\r\n",
        "model.add(LSTM(100))\r\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLN_hTSyi9jh"
      },
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.011)\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', \r\n",
        "             optimizer=opt, \r\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFa_aszyoacq",
        "outputId": "ece5d7f2-1f90-4c7e-a51b-ccbb1bd49020"
      },
      "source": [
        "batch_size = 60\r\n",
        "num_epochs = 3\r\n",
        "\r\n",
        "y_dev = np.array(y_dev)\r\n",
        "y_train = np.array(y_train)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "40/40 [==============================] - 22s 549ms/step - loss: 0.6501 - accuracy: 0.6400 - val_loss: 0.5571 - val_accuracy: 0.6967\n",
            "Epoch 2/3\n",
            "40/40 [==============================] - 22s 542ms/step - loss: 0.3091 - accuracy: 0.8733 - val_loss: 0.5681 - val_accuracy: 0.7500\n",
            "Epoch 3/3\n",
            "40/40 [==============================] - 21s 537ms/step - loss: 0.1176 - accuracy: 0.9654 - val_loss: 0.7064 - val_accuracy: 0.7567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff854db76a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cSPPEOhuFco",
        "outputId": "b6b1ebb9-f590-4177-a01f-122cad4d5216"
      },
      "source": [
        "model.evaluate(X_test, np.array(y_test))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 79ms/step - loss: 0.4875 - accuracy: 0.8227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.48747390508651733, 0.8227424621582031]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeIJwF5cVzEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77074cb3-2cb2-4d3e-c94b-b0604ea9f51b"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "# print(len(y_test))\r\n",
        "\r\n",
        "predictions = model.predict(X_test)  \r\n",
        "# print(predictions)\r\n",
        "\r\n",
        "predictions = np.where(predictions > .5,1,0)\r\n",
        "\r\n",
        "# print(confusion_matrix(np.array(y_test), predictions))\r\n",
        "print(classification_report(np.array(y_test), predictions))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       186\n",
            "           1       0.79      0.73      0.76       113\n",
            "\n",
            "    accuracy                           0.82       299\n",
            "   macro avg       0.81      0.80      0.81       299\n",
            "weighted avg       0.82      0.82      0.82       299\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9iVnxuVzG0"
      },
      "source": [
        ""
      ],
      "execution_count": 74,
      "outputs": []
    }
  ]
}